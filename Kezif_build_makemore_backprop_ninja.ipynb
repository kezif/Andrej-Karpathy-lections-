{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kezif/Andrej-Karpathy-lections-/blob/main/Kezif_build_makemore_backprop_ninja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToK0Tku8PPn"
      },
      "source": [
        "## makemore: becoming a backprop ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "8sFElPqq8PPp"
      },
      "outputs": [],
      "source": [
        "# there no change change in the first several cells from last lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "ChBbac4y8PPq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "id": "x6GhEWW18aCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9e1319-ed40-49c9-a7f7-ef42ed23b3c8"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-18 21:33:18--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.3’\n",
            "\n",
            "\rnames.txt.3           0%[                    ]       0  --.-KB/s               \rnames.txt.3         100%[===================>] 222.80K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-02-18 21:33:18 (37.1 MB/s) - ‘names.txt.3’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "klmu3ZG08PPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2886d9a1-b9f1-46ad-f652-1d61c2e3c0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "BCQomLE_8PPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf35b5a-268d-46d9-b023-f7b40e40e1c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "V_zt2QHr8PPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b17593-04cc-48e1-8ba1-182b636b1e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):  \n",
        "  X, Y = [], []\n",
        "  \n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "eg20-vsg8PPt"
      },
      "outputs": [],
      "source": [
        "# ok biolerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "ZlFLjQyT8PPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35dfe629-20a7-4656-e917-3413485e5e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "QY-y96Y48PPv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C[Xb].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQLskulJnteu",
        "outputId": "728c07fa-94b5-44cc-8f21-ef3c0c6cfbe4"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "8ofj1s6d8PPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af4b324-9b8f-4356-d3d3-85dd0cb880d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3294, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ],
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity  bnraw  \n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = torch.zeros(3,2)\n",
        "a[[0,2], [1]] = 1\n",
        "b = a.view(2,3)\n",
        "c = a.view(torch.uint8)\n",
        "\n",
        "a[0,0] = 99\n",
        "print(a,b,c)  # if view used like reshape arrays data would be the same. stored in one place"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f66btGu1gnzo",
        "outputId": "9c64a297-6ab2-44a2-a089-ce18ba21b8ad"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[99.,  1.],\n",
            "        [ 0.,  0.],\n",
            "        [ 0.,  1.]]) tensor([[99.,  1.,  0.],\n",
            "        [ 0.,  0.,  1.]]) tensor([[  0,   0, 198,  66,   0,   0, 128,  63],\n",
            "        [  0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  0,   0,   0,   0,   0,   0, 128,  63]], dtype=torch.uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = - (a + b + c) / 3\n",
        "#dloss/da = - a/3\n",
        "\n",
        "#logprobs = c[0,0] * n[0], c[0,1] * n[0]\n",
        "#           c[1,0] * n[1], c[1,1] * n[1]      \n",
        "\n",
        "\n",
        "#dprobs/dc[0,0] = n[0]\n",
        "#probs/dc[0,1] = n[0]\n",
        "#dprobs/dc[1,0] = n[1]\n",
        "#dprobs/dc[1,1] = n[1]\n",
        "#dprobs    = n[0], n[0]\n",
        "#            n[1], n[1]\n",
        "'''counts_sum = [\n",
        "    [a + b],\n",
        "    [c + d]\n",
        "]]'''\n",
        "#counts_sum/da = 1\n",
        "#counts.sum(1, keepdims=True)\n",
        "\n",
        "\n",
        "#a(3,2) @ b(2,1)\n",
        "#  a11*b11 + a12*b21\n",
        "#  a12*b11 + a21*b21\n",
        "#  a13*b11 + a31*b21 \n",
        "#d/da\n",
        "\n",
        "# dbnvar\n",
        "#bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# dbnvar = d u**-0.5   *  d  u\n",
        "#               u         d bnvar\n",
        "#   -0.5 * (bnvar + 1e-5) ** -1.5   \n",
        "\n",
        "\n",
        "# bndiff2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
        "# \n",
        "\n",
        "# bndiff\n",
        "# bndiff2 = bndiff**2\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "\n",
        "# hprebn\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "\n",
        "\n",
        "# W1  -> 30x64\n",
        "#dhprebn @ embcat\n",
        "#32x64 32x30\n",
        "\n",
        "# demb\n",
        "# 32 3 10\n",
        "# 32 30\n",
        "\n",
        "# embcat\n",
        "# hprebn = embcat @ W1\n",
        "\n",
        "# C 27x10\n",
        "#demb[Xb]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5wP0wkFsa5qu",
        "outputId": "47e5cdaa-34e3-4fb5-fcee-bc2d33b30753"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'counts_sum = [\\n    [a + b],\\n    [c + d]\\n]]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "mO-8aqxK8PPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2fd1c22-e90e-446c-8042-f321a7b2d5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: False | approximate: True  | maxdiff: 4.889443516731262e-09\n",
            "logits          | exact: False | approximate: True  | maxdiff: 4.889443516731262e-09\n",
            "h               | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "W2              | exact: False | approximate: False | maxdiff: 1.30385160446167e-08\n",
            "b2              | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 2.0954757928848267e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: backprop through the whole thing manually, \n",
        "# backpropagating through exactly all of the variables \n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/n \n",
        "dprobs = dlogprobs * 1 / probs\n",
        "dcounts_sum_inv = (dprobs * counts).sum(1,keepdim=True)\n",
        "dcounts_sum = dcounts_sum_inv * -counts_sum**-2\n",
        "\n",
        "dcounts = dprobs * counts_sum_inv   \n",
        "dcounts +=  dcounts_sum\n",
        "\n",
        "dnorm_logits = dcounts * norm_logits.exp()\n",
        "dlogit_maxes = torch.zeros_like(dnorm_logits)#(dnorm_logits * -1).sum(1,keepdim=True)\n",
        "dlogits = dnorm_logits\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.T.sum(1) \n",
        "dhpreact = dh * (1 - h**2)\n",
        "dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
        "dbnbias = (dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = dhpreact * bngain\n",
        "dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim=True)\n",
        "dbnvar = dbnvar_inv * -0.5 * (bnvar + 1e-5) ** -1.5\n",
        "dbndiff2 = dbnvar * 1/(n-1)\n",
        "dbndiff = dbndiff2 * 2 * bndiff + dbnraw * bnvar_inv\n",
        "dbnmeani = (dbndiff * -1).sum(0, keepdim=True)\n",
        "dhprebn = dbnmeani * 1/n + dbndiff\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn   \n",
        "db1 = dhprebn.sum(0,keepdim=True)\n",
        "demb = dembcat.reshape(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "#dC[Xb] += demb.reshape(dC[Xb].shape)\n",
        "for i in range(Xb.shape[0]):\n",
        "    for j in range(Xb.shape[1]):\n",
        "        idx = Xb[i,j]\n",
        "        dC[idx] += demb[i,j]\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = [1]\n",
        "a = torch.zeros(2,2)\n",
        "a[idx] = -1\n",
        "b = torch.zeros(2,2)\n",
        "b[idx] = torch.ones_like(b[idx]) * -1\n",
        "\n",
        "a == b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDqMDRvBY_SL",
        "outputId": "4af694eb-7bcf-4d07-920a-46113077d02f"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True],\n",
              "        [True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wsj643Q4jM7c"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "ebLtYji_8PPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fdba383-7399-4aa1-d08c-83fa8bd77303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.359736680984497 diff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#probs = counts * counts_sum_inv\n",
        "#\n",
        "#\n",
        "#counts = norm_logits.exp()\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "fig, ax = plt.subplots(1,2,)  # andrej used plotting to show dlogits as image.\n",
        "ax[0].imshow(testlogits2)     # funny enought i've came up with the same idea to look at two diferent implementation befora watching the video\n",
        "ax[1].imshow(testlogits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "g7OFBkeAp8Rb",
        "outputId": "acfd405c-2155-4bb3-f506-cc9f39ed71fa"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9cc57849d0>"
            ]
          },
          "metadata": {},
          "execution_count": 194
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "-gCXbB4C8PPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f35fb5-c199-46af-983e-6cc6f8981c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 6.05359673500061e-09\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "with torch.no_grad():\n",
        "    dlogprobs = torch.zeros_like(logits)\n",
        "    dlogprobs[range(n), Yb] = -1.0/n \n",
        "    soft = F.softmax(logits, dim=1) \n",
        "    testlogits = dlogprobs + dlogprobs.sum(1,keepdim=True) * -soft\n",
        "\n",
        "    testlogits2 = F.softmax(logits, dim=1)   # andrej solution\n",
        "    testlogits2[[range(n), Yb]] -= 1         \n",
        "    testlogits2 /= n\n",
        "    torch.allclose(testlogits, testlogits2)\n",
        "\n",
        "cmp('logits', testlogits2, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n",
        "torch.allclose(testlogits, testlogits2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "hd-MkhB68PPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e37cc3-d368-4e23-8abc-91c69dbefd3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "POdeZSKT8PPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0eaa4d4-eb48-4be9-f795-19bdfb10c826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 1.6298145055770874e-09\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# dhprebn = (2*bndiff) * ((1.0/(n-1))*torch.ones_like(bndiff2) * (-0.5*(bnvar + 1e-5)**-1.5) * (bndiff * dbnraw).sum(0, keepdim=True)) + 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))  # i tried calculating derivative analyticaly as well. Well part of it :)\n",
        "\n",
        "\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnmeani.shape\n",
        "from tqdm.notebook import tqdm, trange"
      ],
      "metadata": {
        "id": "xm81wkb1Mnd5"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "wPy8DhqB8PPz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "3eaebc1031e540f8a85a2e1592086e62",
            "54a53b9abd0c4d0fa9d77e043155f895",
            "0fb68765031b4f8c8294ebf42c6d9a39",
            "6ae0679bd5e745a990703a4d98f1c9ba",
            "e132b8dfc1ed458e9377b5e218f7be60",
            "c2849ee38c9f4883a18b6bbf417951e8",
            "f8876fefb7fa417c847de4e6babb90bf",
            "d8038f1e6f6e4ed0bbea2bfba036c986",
            "97bf0454663b40e9a9f03ca692c3775f",
            "42b456a1cee6462bb755690b965aa59d",
            "6d21de9ff63142c89dde6f25bf379630"
          ]
        },
        "outputId": "9e272e35-f15b-46ec-b65e-18c66f73f55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/200000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3eaebc1031e540f8a85a2e1592086e62"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "    p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "    # kick off optimization\n",
        "    for i in trange(max_steps):\n",
        "\n",
        "        # minibatch construct\n",
        "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "        Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "        # forward pass\n",
        "        emb = C[Xb] # embed the characters into vectors\n",
        "        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "        # Linear layer\n",
        "        hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "        # BatchNorm layer\n",
        "        # -------------------------------------------------------------\n",
        "        bnmean = hprebn.mean(0, keepdim=True)\n",
        "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "        hpreact = bngain * bnraw + bnbias\n",
        "        # -------------------------------------------------------------\n",
        "        # Non-linearity\n",
        "        h = torch.tanh(hpreact) # hidden layer\n",
        "        logits = h @ W2 + b2 # output layer\n",
        "        loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "        # backward pass\n",
        "        #for p in parameters:\n",
        "        #    p.grad = None\n",
        "        #loss.backward() # use this for correctness comparisons, delete it later!\n",
        "        # manual backprop! #swole_doge_meme\n",
        "        # -----------------\n",
        "        # YOUR CODE HERE :)\n",
        "        dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
        "\n",
        "        ##dlogits = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))  # batchnorm\n",
        "\n",
        "        dlogprobs = torch.zeros_like(logits)  # cross entropy\n",
        "        dlogprobs[range(n), Yb] = -1.0/n \n",
        "        soft = F.softmax(logits, dim=1) \n",
        "        dlogits = dlogprobs + dlogprobs.sum(1,keepdim=True) * -soft\n",
        "\n",
        "        #dlogits = F.softmax(logits, dim=1)   # andrej solution\n",
        "        #dlogits[[range(n), Yb]] -= 1         \n",
        "        #dlogits /= n\n",
        "\n",
        "        dh = dlogits @ W2.T  # linear\n",
        "        dW2 = h.T @ dlogits\n",
        "        db2 = dlogits.T.sum(1) \n",
        "\n",
        "        dhpreact = dh * (1 - h**2)  # batch norm\n",
        "        dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
        "        dbnbias = (dhpreact).sum(0, keepdim=True)\n",
        "        dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "        dembcat = dhprebn @ W1.T\n",
        "        dW1 = embcat.T @ dhprebn   \n",
        "        db1 = dhprebn.sum(0)# linear\n",
        "        demb = dembcat.reshape(emb.shape)\n",
        "        dC = torch.zeros_like(C)\n",
        "        #dC[Xb] += demb.reshape(dC[Xb].shape)\n",
        "        for i in range(Xb.shape[0]):\n",
        "            for j in range(Xb.shape[1]):\n",
        "                idx = Xb[i,j]\n",
        "                dC[idx] += demb[i,j]\n",
        "        \n",
        "\n",
        "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "        # -----------------\n",
        "\n",
        "        # update\n",
        "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "        for p, grad in zip(parameters, grads):\n",
        "            #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "            #print(grad.shape)\n",
        "            p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "        # track stats\n",
        "        if i % 10000 == 0: # print every once in a while\n",
        "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "        lossi.append(loss.log10().item())\n",
        "\n",
        "        if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEpI0hMW8PPz"
      },
      "outputs": [],
      "source": [
        "# useful for checking your gradients\n",
        "# for p,g in zip(parameters, grads):\n",
        "#   cmp(str(tuple(p.shape)), g, p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "KImLWNoh8PP0"
      },
      "outputs": [],
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "6aFnP_Zc8PP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09ccf8f-68f3-47ae-a0d2-d4f85889ed00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.120304822921753\n",
            "val 2.165170907974243\n"
          ]
        }
      ],
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esWqmhyj8PP1"
      },
      "outputs": [],
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "xHeQNv3s8PP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252ffbc6-5913-4d84-a53e-d83af7ea0de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "carmahza.\n",
            "jahmarik.\n",
            "miri.\n",
            "tell.\n",
            "skardon.\n",
            "jazon.\n",
            "nadhery.\n",
            "caireei.\n",
            "nellara.\n",
            "chriha.\n",
            "kaleigh.\n",
            "ham.\n",
            "joce.\n",
            "quinn.\n",
            "shoin.\n",
            "ania.\n",
            "bison.\n",
            "elo.\n",
            "dearisi.\n",
            "jaxen.\n"
          ]
        }
      ],
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    \n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "    \n",
        "    print(''.join(itos[i] for i in out))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3eaebc1031e540f8a85a2e1592086e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54a53b9abd0c4d0fa9d77e043155f895",
              "IPY_MODEL_0fb68765031b4f8c8294ebf42c6d9a39",
              "IPY_MODEL_6ae0679bd5e745a990703a4d98f1c9ba"
            ],
            "layout": "IPY_MODEL_e132b8dfc1ed458e9377b5e218f7be60"
          }
        },
        "54a53b9abd0c4d0fa9d77e043155f895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2849ee38c9f4883a18b6bbf417951e8",
            "placeholder": "​",
            "style": "IPY_MODEL_f8876fefb7fa417c847de4e6babb90bf",
            "value": "100%"
          }
        },
        "0fb68765031b4f8c8294ebf42c6d9a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8038f1e6f6e4ed0bbea2bfba036c986",
            "max": 200000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97bf0454663b40e9a9f03ca692c3775f",
            "value": 200000
          }
        },
        "6ae0679bd5e745a990703a4d98f1c9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42b456a1cee6462bb755690b965aa59d",
            "placeholder": "​",
            "style": "IPY_MODEL_6d21de9ff63142c89dde6f25bf379630",
            "value": " 200000/200000 [08:48&lt;00:00, 404.24it/s]"
          }
        },
        "e132b8dfc1ed458e9377b5e218f7be60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2849ee38c9f4883a18b6bbf417951e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8876fefb7fa417c847de4e6babb90bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8038f1e6f6e4ed0bbea2bfba036c986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97bf0454663b40e9a9f03ca692c3775f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42b456a1cee6462bb755690b965aa59d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d21de9ff63142c89dde6f25bf379630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}